{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Indigo_Chatbot**"
      ],
      "metadata": {
        "id": "itPXcA7OOslq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install LangSmith**"
      ],
      "metadata": {
        "id": "8HfuAO2pGXcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Sets up LangSmith tooling so you can trace, debug, and compare runs.\n",
        "Ensures your project has experiment tracking from the very first step."
      ],
      "metadata": {
        "id": "xYAn2ELMGeBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langsmith\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0Ht0j4UoRVB",
        "outputId": "d9df8ef9-cfb3-49e0-e0b9-5821af9dc9f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.1.147)\n",
            "Collecting langsmith\n",
            "  Using cached langsmith-0.4.13-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.11.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Using cached langsmith-0.4.13-py3-none-any.whl (372 kB)\n",
            "Installing collected packages: langsmith\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.1.147\n",
            "    Uninstalling langsmith-0.1.147:\n",
            "      Successfully uninstalled langsmith-0.1.147\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.2.6 requires langsmith<0.2.0,>=0.1.17, but you have langsmith 0.4.13 which is incompatible.\n",
            "langchain-core 0.2.43 requires langsmith<0.2.0,>=0.1.112, but you have langsmith 0.4.13 which is incompatible.\n",
            "langchain-community 0.2.6 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.4.13 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langsmith-0.4.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configure LangSmith Project & Tracing**"
      ],
      "metadata": {
        "id": "g3a7rDLDGvuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines LANGCHAIN_API_KEY, enables tracing v2, and names the project.\n",
        "Gives you instant observability into chains, prompts, and model calls."
      ],
      "metadata": {
        "id": "dkGViJfnG2Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"  # replace with your key\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Indigo-RAG-Streamlit\"  # name your project\n"
      ],
      "metadata": {
        "id": "DR9eg7pcoiIQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Core Dependencies**"
      ],
      "metadata": {
        "id": "5vjAs0wyG7TR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pins and installs LangChain, ChromaDB, Sentence-Transformers, pypdf, Groq, and friends.\n",
        "Locks versions for reproducible environments and fewer “works-on-my-machine” issues."
      ],
      "metadata": {
        "id": "VivVI-jAG9Qz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyDII1bQVnbe",
        "outputId": "daa80077-1a47-4859-eb48-f3b9463d96c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/321.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m266.2/321.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "streamlit 1.35.0 requires protobuf<5,>=3.20, but you have protobuf 6.31.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.31.1 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Core libs (no faiss)\n",
        "!pip -q install \"numpy<2.0\" pypdf==4.2.0\n",
        "!pip -q install langchain==0.2.6 langchain-community==0.2.6\n",
        "!pip -q install chromadb==0.5.3 sentence-transformers==3.0.1\n",
        "!pip -q install groq==0.9.0 tiktoken==0.7.0 python-dotenv==1.0.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Groq API Key**"
      ],
      "metadata": {
        "id": "o4Kx8Gj7HEqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds GROQ_API_KEY to the environment so Llama3 endpoints can be called securely.\n",
        "One place to manage credentials without hard-coding them in the app."
      ],
      "metadata": {
        "id": "CujWDJIgHGPq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVH2GvqLWU9X",
        "outputId": "112b48aa-bbbe-4630-b011-65627216dff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GROQ key set: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "print(\"GROQ key set:\", bool(os.environ.get(\"GROQ_API_KEY\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build RAG Engine (rag_core.py)**"
      ],
      "metadata": {
        "id": "b4w4Cf6lHJZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writes a reusable module: loaders, chunking, embeddings, Chroma, RAG answer, summarize, quiz, and logging.\n",
        "Keeps the core retrieval + generation logic clean, testable, and separate from the UI."
      ],
      "metadata": {
        "id": "Bu7AtGVZHMOq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9a_5iwYWeJd",
        "outputId": "de13b376-473c-4228-d0e6-097976363774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_core.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile rag_core.py\n",
        "import os, json, requests, sqlite3, time, random, hashlib, shutil, re\n",
        "from typing import List\n",
        "\n",
        "# ───────────────────── LangSmith tracking (kept as you wanted) ─────────────────────\n",
        "# NOTE: If you push to a public repo, rotate this key later.\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]    = \"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\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"]    = \"Indigo-RAG-Streamlit\"\n",
        "\n",
        "from langsmith import traceable  # noqa: F401\n",
        "\n",
        "# Runtime safety to avoid GPU meta-tensor issues in some Colab images\n",
        "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "\n",
        "from pypdf import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# ───────────────────── Config ─────────────────────\n",
        "# Speed toggle: to use a lighter, very fast embedder:\n",
        "#   os.environ[\"EMBED_MODEL\"] = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"thenlper/gte-large\")\n",
        "\n",
        "# Primary model (can be overridden by sidebar via env)\n",
        "GROQ_MODEL_PRIMARY = os.getenv(\"GROQ_MODEL_OVERRIDE\", \"llama-3.3-70b-versatile\")\n",
        "\n",
        "# Fallback model list (first try PRIMARY, then these). You can override order via env:\n",
        "#   GROQ_MODEL_CANDIDATES=\"llama-3.1-8b-instant,deepseek-r1-distill-llama-70b,llama-3.1-70b-versatile\"\n",
        "DEFAULT_CANDIDATES = [\n",
        "    \"llama-3.3-70b-versatile\",\n",
        "    \"deepseek-r1-distill-llama-70b\",\n",
        "    \"llama-3.1-70b-versatile\",\n",
        "    \"llama-3.1-8b-instant\",\n",
        "    \"llama3-70b-8192\",   # legacy aliases (if present on your account)\n",
        "    \"llama3-8b-8192\"\n",
        "]\n",
        "_env_candidates = [m.strip() for m in os.getenv(\"GROQ_MODEL_CANDIDATES\", \"\").split(\",\") if m.strip()]\n",
        "GROQ_MODEL_CANDIDATES = [GROQ_MODEL_PRIMARY] + [\n",
        "    m for m in (_env_candidates or DEFAULT_CANDIDATES) if m != GROQ_MODEL_PRIMARY\n",
        "]\n",
        "\n",
        "# Groq OpenAI-compatible base\n",
        "GROQ_BASE = os.getenv(\"GROQ_BASE_URL\", \"https://api.groq.com/openai/v1\")\n",
        "\n",
        "# ───────────────────── Embeddings / Chroma ─────────────────────\n",
        "def _pick_device():\n",
        "    \"\"\"Prefer CUDA if actually available; otherwise CPU.\"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            return \"cuda\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"cpu\"\n",
        "\n",
        "def get_embeddings():\n",
        "    \"\"\"\n",
        "    Load sentence-transformer; prefer GPU for speed, else fallback to CPU.\n",
        "    Use bigger batch + normalization for faster, stable cosine similarity.\n",
        "    \"\"\"\n",
        "    device = _pick_device()  # \"cuda\" or \"cpu\"\n",
        "    model_name = os.getenv(\"EMBED_MODEL\", EMBED_MODEL)\n",
        "    model_kwargs = {\"device\": device}\n",
        "    encode_kwargs = {\"normalize_embeddings\": True, \"batch_size\": 64}\n",
        "    return HuggingFaceEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs,\n",
        "    )\n",
        "\n",
        "def chunk_texts(docs, chunk_size=1800, chunk_overlap=150):\n",
        "    \"\"\"Fewer, larger chunks reduce embedding calls → faster indexing.\"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    return splitter.split_documents(docs)\n",
        "\n",
        "def load_pdfs_as_docs(file_paths: List[str]):\n",
        "    from langchain_core.documents import Document\n",
        "    out = []\n",
        "    for path in file_paths:\n",
        "        r = PdfReader(path)\n",
        "        for i, page in enumerate(r.pages, start=1):\n",
        "            t = (page.extract_text() or \"\").strip()\n",
        "            if t:\n",
        "                out.append(Document(\n",
        "                    page_content=\" \".join(t.split()),\n",
        "                    metadata={\"source\": os.path.basename(path), \"page\": i}\n",
        "                ))\n",
        "    return out\n",
        "\n",
        "def _safe_rmtree(path: str):\n",
        "    try:\n",
        "        if os.path.isdir(path):\n",
        "            shutil.rmtree(path, ignore_errors=True)\n",
        "        elif os.path.exists(path):\n",
        "            os.remove(path)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def build_or_update_chroma(chunks, persist_dir=\"chroma_db\", collection_name=\"docs\", reset: bool = False):\n",
        "    \"\"\"\n",
        "    Build or update a Chroma index.\n",
        "    - reset=True: nuke the existing DB (fastest / no-duplicates).\n",
        "    - reset=False: append to the existing DB.\n",
        "    \"\"\"\n",
        "    emb = get_embeddings()\n",
        "    os.makedirs(persist_dir, exist_ok=True)\n",
        "\n",
        "    if reset:\n",
        "        # Clean the directory to rebuild from scratch\n",
        "        for fn in os.listdir(persist_dir):\n",
        "            _safe_rmtree(os.path.join(persist_dir, fn))\n",
        "\n",
        "    # Chroma.from_documents() creates or rebuilds fresh; faster on clean builds\n",
        "    vs = Chroma.from_documents(chunks, emb, persist_directory=persist_dir, collection_name=collection_name)\n",
        "    vs.persist()\n",
        "    return vs\n",
        "\n",
        "def load_chroma(persist_dir=\"chroma_db\", collection_name=\"docs\"):\n",
        "    emb = get_embeddings()\n",
        "    return Chroma(persist_directory=persist_dir, collection_name=collection_name, embedding_function=emb)\n",
        "\n",
        "# ───────────────────── Groq HTTP client ─────────────────────\n",
        "def _sleep_backoff(attempt: int) -> None:\n",
        "    \"\"\"Jittered exponential backoff: 1s, 2s, 4s, 8s, 12s max.\"\"\"\n",
        "    time.sleep(min(12, 2 ** attempt) + random.random())\n",
        "\n",
        "def _groq_post(payload: dict, api_key: str, base_url: str):\n",
        "    url = f\"{base_url}/chat/completions\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\", \"Accept\": \"application/json\"}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "\n",
        "def groq_complete(prompt: str, temperature: float = 0.2, max_tokens: int = 512) -> str:\n",
        "    \"\"\"\n",
        "    Robust Groq call with:\n",
        "      • correct base URL (OpenAI-compatible),\n",
        "      • POST /chat/completions,\n",
        "      • retries for transient 429/5xx,\n",
        "      • auto-fallback across multiple models if TPD exhausted on one.\n",
        "    \"\"\"\n",
        "    api_key = os.getenv(\"GROQ_API_KEY\", \"\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"GROQ_API_KEY not set\")\n",
        "\n",
        "    base_url = os.getenv(\"GROQ_BASE_URL\", GROQ_BASE)\n",
        "    primary = os.getenv(\"GROQ_MODEL_OVERRIDE\", GROQ_MODEL_PRIMARY)\n",
        "\n",
        "    candidates = [primary] + [m for m in GROQ_MODEL_CANDIDATES if m != primary]\n",
        "    last_err = None\n",
        "\n",
        "    for model in candidates:\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"temperature\": float(temperature),\n",
        "            \"max_tokens\": int(max_tokens),\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a concise, grounded assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            \"stream\": False,\n",
        "        }\n",
        "\n",
        "        # Retry loop per model (handles RPM/TPM and 5xx). TPD → try next model immediately.\n",
        "        for attempt in range(6):\n",
        "            try:\n",
        "                r = _groq_post(payload, api_key, base_url)\n",
        "\n",
        "                if r.status_code == 429:\n",
        "                    # Distinguish TPD (daily) vs transient rate limits\n",
        "                    try:\n",
        "                        j = r.json()\n",
        "                        emsg = (j.get(\"error\") or {}).get(\"message\", \"\")\n",
        "                        ecode = (j.get(\"error\") or {}).get(\"code\", \"\")\n",
        "                    except Exception:\n",
        "                        emsg, ecode = r.text, \"\"\n",
        "                    # Tokens-per-day hard cap → switch model\n",
        "                    if \"tokens per day\" in emsg.lower() or ecode == \"rate_limit_exceeded\":\n",
        "                        last_err = f\"TPD exhausted for `{model}`: {emsg}\"\n",
        "                        break\n",
        "                    # Transient per-minute/TPM → backoff + retry same model\n",
        "                    _sleep_backoff(attempt); last_err = emsg; continue\n",
        "\n",
        "                if r.status_code in (500, 502, 503, 504):\n",
        "                    _sleep_backoff(attempt); last_err = f\"HTTP {r.status_code}\"; continue\n",
        "\n",
        "                r.raise_for_status()\n",
        "                data = r.json()\n",
        "                return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "            except requests.RequestException as e:\n",
        "                s = str(e)\n",
        "                if any(code in s for code in [\"429\", \"500\", \"502\", \"503\", \"504\"]):\n",
        "                    _sleep_backoff(attempt); last_err = s; continue\n",
        "                if \"Unknown request URL\" in s or \"unknown_url\" in s:\n",
        "                    raise RuntimeError(\n",
        "                        f\"Groq base URL looks wrong: {base_url}. \"\n",
        "                        \"Expected 'https://api.groq.com/openai/v1'. Original error: \" + s\n",
        "                    )\n",
        "                raise\n",
        "\n",
        "        # If we reached here, likely TPD on this model → try next candidate\n",
        "        continue\n",
        "\n",
        "    # All candidates failed/exhausted\n",
        "    raise RuntimeError(f\"All Groq models exhausted/limited. Last error: {last_err}\")\n",
        "\n",
        "# ───────────────────── Helpers ─────────────────────\n",
        "def _format_citations(docs) -> str:\n",
        "    parts = []\n",
        "    for i, d in enumerate(docs, start=1):\n",
        "        src = d.metadata.get(\"source\",\"unknown\"); pg = d.metadata.get(\"page\",\"-\")\n",
        "        parts.append(f\"[S{i}] ({src} p.{pg})\\n{d.page_content}\")\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "def _coerce_json_array(raw: str):\n",
        "    \"\"\"\n",
        "    Try to extract a valid JSON array from a messy LLM output.\n",
        "    - strips code fences\n",
        "    - finds the first [...] block\n",
        "    - best-effort single→double quote fix (last resort)\n",
        "    \"\"\"\n",
        "    s = raw.strip()\n",
        "    # remove common code fences & leading prose like ```json ... ```\n",
        "    s = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", s, flags=re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "    # grab the first bracketed array\n",
        "    start = s.find(\"[\")\n",
        "    end   = s.rfind(\"]\")\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "        snippet = s[start:end+1]\n",
        "        try:\n",
        "            return json.loads(snippet)\n",
        "        except Exception:\n",
        "            pass  # fall through\n",
        "\n",
        "    # final attempt: replace single quotes with double quotes (best-effort)\n",
        "    s2 = s.replace(\"'\", '\"')\n",
        "    start = s2.find(\"[\")\n",
        "    end   = s2.rfind(\"]\")\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "        snippet = s2[start:end+1]\n",
        "        return json.loads(snippet)  # may still raise\n",
        "\n",
        "    raise ValueError(\"Could not coerce JSON array from model output.\")\n",
        "\n",
        "# ───────────────────── RAG tasks ─────────────────────\n",
        "def rag_answer(vs, question: str, k: int = 5, temperature: float = 0.2, max_tokens: int = 512):\n",
        "    docs = vs.similarity_search(question, k=k)\n",
        "    ctx = _format_citations(docs)\n",
        "    prompt = f\"\"\"Answer strictly from the context. If unsure, say you don't know.\n",
        "Be EXHAUSTIVE: include every step, rule, exception, and prerequisite you find.\n",
        "Cite sources inline like [S1], [S2] (they map to (source,page)). Use numbered lists.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context:\n",
        "{ctx}\n",
        "\n",
        "Answer (exhaustive with citations):\"\"\"\n",
        "    ans = groq_complete(prompt, temperature=temperature, max_tokens=max_tokens)\n",
        "    cites = [{\"slot\": f\"S{i+1}\", \"source\": d.metadata.get(\"source\"), \"page\": d.metadata.get(\"page\")} for i,d in enumerate(docs)]\n",
        "    return {\"answer\": ans, \"citations\": cites}\n",
        "\n",
        "def rag_answer_exhaustive(vs, question: str, k: int = 40, batch: int = 8,\n",
        "                          temperature: float = 0.0, max_tokens: int = 900):\n",
        "    docs = vs.similarity_search(question, k=k)\n",
        "    partials = []\n",
        "    for i in range(0, len(docs), batch):\n",
        "        group = docs[i:i+batch]\n",
        "        ctx = _format_citations(group)\n",
        "        prompt = f\"\"\"You will answer a complex question using ONLY this batch of context.\n",
        "List EVERY relevant step/detail you can find. Use [S#] citations for each bullet.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context:\n",
        "{ctx}\n",
        "\n",
        "Partial answer (bulleted with citations):\"\"\"\n",
        "        partials.append(groq_complete(prompt, temperature=temperature, max_tokens=max_tokens))\n",
        "    merged_ctx = \"\\n\\n\".join([f\"[P{i+1}] {p}\" for i, p in enumerate(partials)])\n",
        "    final_prompt = f\"\"\"You are merging partial answers into ONE exhaustive answer.\n",
        "Remove duplicates, keep ordering logical, and preserve [S#] citations from partials.\n",
        "If something conflicts, note both and cite them.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Partials:\n",
        "{merged_ctx}\n",
        "\n",
        "Final exhaustive answer (numbered list with [S#] citations):\"\"\"\n",
        "    final = groq_complete(final_prompt, temperature=0.0, max_tokens=1200)\n",
        "    cites = [{\"slot\": f\"S{i+1}\", \"source\": d.metadata.get(\"source\"), \"page\": d.metadata.get(\"page\")}\n",
        "             for i, d in enumerate(docs[:min(15, len(docs))])]\n",
        "    return {\"answer\": final, \"citations\": cites}\n",
        "\n",
        "def summarize(vs, topic_hint: str = \"overview\", k: int = 20, max_tokens: int = 700):\n",
        "    docs = vs.similarity_search(topic_hint, k=k)\n",
        "    ctx = _format_citations(docs)\n",
        "    prompt = f\"\"\"Summarize the following content clearly and concisely as bullet points with key steps/terms.\n",
        "\n",
        "Context:\n",
        "{ctx}\n",
        "\n",
        "Summary:\"\"\"\n",
        "    return groq_complete(prompt, temperature=0.2, max_tokens=max_tokens)\n",
        "\n",
        "def quiz(vs, topic_hint: str = \"\", num: int = 5, k: int = 15, max_tokens: int = 900):\n",
        "    q = topic_hint if topic_hint.strip() else \"important concepts and procedures\"\n",
        "    docs = vs.similarity_search(q, k=k)\n",
        "    ctx = _format_citations(docs)\n",
        "\n",
        "    # Stricter, JSON-only instructions\n",
        "    prompt = f\"\"\"Generate {num} MCQs from the context.\n",
        "Rules:\n",
        "- USE ONLY the context.\n",
        "- Output MUST be a JSON array (no markdown, no prose), exactly like:\n",
        "[\n",
        "  {{\"question\":\"...\",\"options\":[\"A\",\"B\",\"C\",\"D\"],\"answer\":\"A\",\"why\":\"1-line rationale\"}},\n",
        "  ...\n",
        "]\n",
        "- \"options\" must have 4 strings. \"answer\" must be exactly one of the options.\n",
        "\n",
        "Context:\n",
        "{ctx}\n",
        "\"\"\"\n",
        "\n",
        "    # Low temperature to reduce chatty outputs\n",
        "    raw = groq_complete(prompt, temperature=0.0, max_tokens=max_tokens)\n",
        "\n",
        "    # Try strict parse … then tolerant parse … otherwise return raw\n",
        "    try:\n",
        "        data = json.loads(raw)\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # tolerant parse\n",
        "    try:\n",
        "        data = _coerce_json_array(raw)\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # if still not JSON, surface the raw for debugging (UI shows it)\n",
        "    return [{\"raw\": raw}]\n",
        "\n",
        "# ───────────────────── SQLite logger ─────────────────────\n",
        "def init_logger(db_path: str = \"rag_logs.db\"):\n",
        "    con = sqlite3.connect(db_path)\n",
        "    cur = con.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS interactions (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        ts INTEGER,\n",
        "        session_id TEXT,\n",
        "        user TEXT,\n",
        "        question TEXT,\n",
        "        mode TEXT,\n",
        "        model TEXT,\n",
        "        top_k INTEGER,\n",
        "        max_tokens INTEGER,\n",
        "        temperature REAL,\n",
        "        answer TEXT,\n",
        "        citations TEXT,\n",
        "        error TEXT\n",
        "    )\"\"\")\n",
        "    con.commit(); con.close()\n",
        "\n",
        "def log_interaction(session_id: str, user: str, question: str, mode: str,\n",
        "                    model: str, top_k: int, max_tokens: int, temperature: float,\n",
        "                    answer: str, citations: list, error: str | None = None,\n",
        "                    db_path: str = \"rag_logs.db\"):\n",
        "    con = sqlite3.connect(db_path)\n",
        "    cur = con.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        INSERT INTO interactions\n",
        "        (ts, session_id, user, question, mode, model, top_k, max_tokens, temperature, answer, citations, error)\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "    \"\"\", (\n",
        "        int(time.time()), session_id, user, question, mode, model, top_k, max_tokens, float(temperature),\n",
        "        answer, json.dumps(citations or []), error\n",
        "    ))\n",
        "    con.commit(); con.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **App Skeleton (app.py) — Full UI with StreamLit including CSS**"
      ],
      "metadata": {
        "id": "7A8hQOzVHabp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates the initial Streamlit app wiring to import rag_core utilities.\n",
        "Establishes the structure for data flow between upload → index → query → answer."
      ],
      "metadata": {
        "id": "LcCd40NjHpiH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XpLBLcHW3tP",
        "outputId": "d556ee65-1fa4-4b54-94b9-72cbcd04cfca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import os, json, tempfile\n",
        "import streamlit as st\n",
        "\n",
        "from rag_core import (\n",
        "    load_pdfs_as_docs, chunk_texts, build_or_update_chroma, load_chroma,\n",
        "    rag_answer, rag_answer_exhaustive, summarize, quiz,\n",
        "    init_logger, log_interaction\n",
        ")\n",
        "\n",
        "# ---------- Config ----------\n",
        "BOT_NAME = \"IndiGo RAG Bot\"\n",
        "HERO_IMG = \"https://img.freepik.com/free-vector/chatbot-chat-message-vectorart_78370-4104.jpg?semt=ais_hybrid&w=740&q=80\"\n",
        "\n",
        "# ---------- Page setup ----------\n",
        "st.set_page_config(page_title=f\"{BOT_NAME} (Chroma + Groq)\", page_icon=\"✈️\", layout=\"wide\")\n",
        "\n",
        "# ---------- Light Blue / Deep Blue with Yellow Accent Theme ----------\n",
        "PALETTE = {\n",
        "    \"bg1\": \"#e6f2ff\", \"bg2\": \"#b3d9ff\", \"card\": \"#ffffff\", \"text\": \"#0d1b2a\",\n",
        "    \"muted\": \"#4f5d75\", \"brand\": \"#0077b6\", \"brand2\": \"#00b4d8\", \"accent\": \"#ffcc00\",\n",
        "    \"success\": \"#10b981\", \"danger\": \"#ef4444\",\n",
        "}\n",
        "THEME_CSS = f\"\"\"\n",
        "<style>\n",
        "html, body, [data-testid=\"stAppViewContainer\"] {{\n",
        "  background: linear-gradient(120deg, {PALETTE['bg1']}, {PALETTE['bg2']});\n",
        "  color: {PALETTE['text']};\n",
        "}}\n",
        "[data-testid=\"stHeader\"] {{ background: rgba(0,0,0,0); }}\n",
        ".block-container {{\n",
        "  padding-top: 1.2rem; padding-bottom: 2rem;\n",
        "  background: {PALETTE['card']};\n",
        "  border-radius: 16px; box-shadow: 0 10px 40px rgba(0,0,0,.15);\n",
        "  border: 1px solid rgba(0,0,0,.08);\n",
        "}}\n",
        "\n",
        "[data-testid=\"stSidebar\"] {{ background: {PALETTE['brand']}; }}\n",
        "[data-testid=\"stSidebar\"] .stMarkdown, [data-testid=\"stSidebar\"] label,\n",
        "[data-testid=\"stSidebar\"] h1,[data-testid=\"stSidebar\"] h2,[data-testid=\"stSidebar\"] h3 {{ color: #fff !important; }}\n",
        "[data-testid=\"stSidebar\"] input, [data-testid=\"stSidebar\"] textarea,\n",
        "[data-testid=\"stSidebar\"] .stTextInput > div > div > input,\n",
        "[data-testid=\"stSidebar\"] .stNumberInput input, [data-testid=\"stSidebar\"] .stSelectbox > div,\n",
        "[data-testid=\"stSidebar\"] .stFileUploader, [data-testid=\"stSidebar\"] .stSlider {{\n",
        "  background: #ffffff !important; color: {PALETTE['text']} !important;\n",
        "  border: 1px solid {PALETTE['brand2']} !important; border-radius: 10px !important;\n",
        "}}\n",
        "[data-testid=\"stSidebar\"] [role=\"listbox\"] *, [data-testid=\"stSidebar\"] [data-baseweb=\"select\"] * {{\n",
        "  color: {PALETTE['text']} !important;\n",
        "}}\n",
        "[data-testid=\"stSidebar\"] svg {{ color: {PALETTE['text']} !important; fill: {PALETTE['text']} !important; }}\n",
        "\n",
        "div[data-testid=\"stSlider\"] label, div[data-testid=\"stSlider\"] span,\n",
        "div[data-testid=\"stTickBarMin\"], div[data-testid=\"stTickBarMax\"] {{ color: {PALETTE['text']} !important; }}\n",
        "div[data-baseweb=\"slider\"] [role=\"slider\"]{{ background:#f59e0b !important; border:2px solid {PALETTE['text']} !important; }}\n",
        "div[data-baseweb=\"slider\"] .rc-slider-track, div[data-baseweb=\"slider\"] .rc-slider-rail {{ background:#0ea5e9 !important; }}\n",
        "\n",
        "[data-testid=\"stSidebar\"] .stButton > button {{\n",
        "  background: linear-gradient(90deg, #ef4444, #f97316);\n",
        "  color: white; border: 0; padding: .55rem 1rem; border-radius: 10px; font-weight: 600;\n",
        "  box-shadow: 0 6px 18px rgba(0,0,0,.15);\n",
        "}}\n",
        "[data-testid=\"stSidebar\"] .stButton > button:hover {{ filter: brightness(1.05); }}\n",
        "\n",
        "h1, h2, h3, h4 {{ color: {PALETTE['text']}; }}\n",
        "input, textarea, .stTextInput > div > div > input {{\n",
        "  background: {PALETTE['bg1']} !important; color: {PALETTE['text']} !important;\n",
        "  border: 1px solid {PALETTE['brand']} !important; border-radius: 10px !important;\n",
        "}}\n",
        ".stSelectbox, .stNumberInput, .stFileUploader, .stSlider, .stRadio, .stCheckbox label {{ color: {PALETTE['text']} !important; }}\n",
        "\n",
        ".stButton > button {{\n",
        "  background: linear-gradient(90deg, {PALETTE['brand']}, {PALETTE['brand2']});\n",
        "  color: white; border: 0; padding: .55rem 1rem; border-radius: 10px; font-weight: 600;\n",
        "  box-shadow: 0 6px 18px rgba(0,0,0,.15);\n",
        "}}\n",
        ".stButton > button:hover {{ filter: brightness(1.05); }}\n",
        "[data-testid=\"baseButton-secondary\"] {{ background: {PALETTE['accent']} !important; color: {PALETTE['text']} !important; border: none !important; }}\n",
        "\n",
        ".streamlit-expanderHeader {{ color: {PALETTE['text']} !important; }}\n",
        "code, pre {{ background: {PALETTE['bg1']} !important; color: {PALETTE['text']} !important; }}\n",
        "\n",
        ".chat-bubble-user {{ background: rgba(0,183,255,.12); border: 1px solid {PALETTE['brand2']};\n",
        "  padding: 12px 14px; border-radius: 12px; margin: 8px 0; color: {PALETTE['text']}; }}\n",
        ".chat-bubble-assistant {{ background: rgba(255,204,0,.10); border: 1px solid {PALETTE['accent']};\n",
        "  padding: 12px 14px; border-radius: 12px; margin: 8px 0; color: {PALETTE['text']}; }}\n",
        "\n",
        ".hero-img {{\n",
        "  width: 92px; height: 92px; border-radius: 18px; object-fit: cover;\n",
        "  border: 2px solid {PALETTE['brand2']}; box-shadow: 0 10px 30px rgba(0,0,0,.15);\n",
        "}}\n",
        ".hero-title {{ font-size: 26px; font-weight: 800; letter-spacing:.2px; }}\n",
        ".hero-sub {{ color: {PALETTE['muted']}; font-size: 13px; }}\n",
        ".hr-soft {{ border-top: 1px solid rgba(0,0,0,.08); margin: .5rem 0 1rem; }}\n",
        "</style>\n",
        "\"\"\"\n",
        "st.markdown(THEME_CSS, unsafe_allow_html=True)\n",
        "\n",
        "# ---------- Hero ----------\n",
        "st.markdown(\n",
        "    f\"\"\"\n",
        "    <div style=\"text-align: center; padding: 12px 6px 4px;\">\n",
        "        <img src=\"{HERO_IMG}\" alt=\"Bot Image\" style=\"width: 200px; border-radius: 16px; margin-bottom: 10px; box-shadow: 0 10px 30px rgba(0,0,0,.20);\" />\n",
        "        <h1 style=\"margin-bottom: 0;\">✈️ {BOT_NAME}</h1>\n",
        "        <p style=\"color: #274c77; font-size: 15px;\">Multi-PDF → Chroma → Q&amp;A / Chat / Summary / Quiz (Groq)</p>\n",
        "    </div>\n",
        "    <hr class=\"hr-soft\">\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "# ---------- Session defaults ----------\n",
        "if \"vs_ready\" not in st.session_state: st.session_state.vs_ready = False\n",
        "if \"messages\" not in st.session_state: st.session_state.messages = []\n",
        "if \"nav\" not in st.session_state: st.session_state.nav = \"📥 Build Index\"\n",
        "if \"persist_dir\" not in st.session_state: st.session_state.persist_dir = \"chroma_db\"\n",
        "if \"collection\" not in st.session_state: st.session_state.collection = \"indigo_docs\"\n",
        "if \"top_k\" not in st.session_state: st.session_state.top_k = 12\n",
        "if \"temperature\" not in st.session_state: st.session_state.temperature = 0.1\n",
        "if \"max_tokens\" not in st.session_state: st.session_state.max_tokens = 2048\n",
        "if \"username\" not in st.session_state: st.session_state.username = \"user\"\n",
        "\n",
        "# ---------- Logger ----------\n",
        "init_logger(\"rag_logs.db\")\n",
        "\n",
        "# ---------- Sidebar ----------\n",
        "with st.sidebar:\n",
        "    st.header(\"Settings\")\n",
        "    with st.form(\"settings_form\"):\n",
        "        groq_key_in = st.text_input(\"GROQ API Key (session only)\", type=\"password\", value=os.environ.get(\"GROQ_API_KEY\",\"\"))\n",
        "        model_choice = st.selectbox(\"Groq model\", [\"llama-3.3-70b-versatile\",\"deepseek-r1-distill-llama-70b\",\"llama-3.1-70b-versatile\",\"llama-3.1-8b-instant\"], index=0)\n",
        "\n",
        "        persist_dir = st.text_input(\"Chroma persist dir\", value=st.session_state.persist_dir)\n",
        "        collection  = st.text_input(\"Collection name\", value=st.session_state.collection)\n",
        "        top_k       = st.slider(\"Top-K retrieved chunks\", 2, 60, st.session_state.top_k)\n",
        "        temperature = st.slider(\"LLM Temperature\", 0.0, 1.0, st.session_state.temperature)\n",
        "        st.caption(f\"Temp value: **{st.session_state.get('temperature', temperature):.2f}**\")\n",
        "        max_tokens  = st.slider(\"Max tokens\", 256, 4096, st.session_state.max_tokens, step=64)\n",
        "        st.caption(f\"Max tokens value: **{st.session_state.get('max_tokens', max_tokens)}**\")\n",
        "\n",
        "        nav = st.radio(\n",
        "            \"Go to\",\n",
        "            [\"📥 Build Index\", \"❓ Q&A\", \"💬 Document Chat\", \"📝 Summarize\", \"🧩 Quiz\"],\n",
        "            index=[\"📥 Build Index\",\"❓ Q&A\",\"💬 Document Chat\",\"📝 Summarize\",\"🧩 Quiz\"].index(st.session_state.nav)\n",
        "        )\n",
        "\n",
        "        submitted = st.form_submit_button(\"Apply\")\n",
        "        if submitted:\n",
        "            if groq_key_in:\n",
        "                os.environ[\"GROQ_API_KEY\"] = groq_key_in.strip()\n",
        "            os.environ[\"GROQ_MODEL_OVERRIDE\"] = model_choice\n",
        "\n",
        "            st.session_state.persist_dir = persist_dir\n",
        "            st.session_state.collection  = collection\n",
        "            st.session_state.top_k       = top_k\n",
        "            st.session_state.temperature = temperature\n",
        "            st.session_state.max_tokens  = max_tokens\n",
        "            st.session_state.nav         = nav\n",
        "\n",
        "    st.caption(\"Key status: \" + (\"✅ set\" if os.environ.get(\"GROQ_API_KEY\") else \"❌ missing\"))\n",
        "    st.caption(\"Embeddings: \" + os.environ.get(\"EMBED_MODEL\", \"thenlper/gte-large\") + \" (GPU if available)\")\n",
        "\n",
        "# ---------- Active settings ----------\n",
        "persist_dir = st.session_state.persist_dir\n",
        "collection  = st.session_state.collection\n",
        "top_k       = st.session_state.top_k\n",
        "temperature = st.session_state.temperature\n",
        "max_tokens  = st.session_state.max_tokens\n",
        "active_page = st.session_state.nav\n",
        "\n",
        "def need_vs():\n",
        "    if not st.session_state.vs_ready:\n",
        "        st.warning(\"No vector DB loaded yet. Build or Load an index first.\")\n",
        "        st.stop()\n",
        "\n",
        "# ---------- Pages ----------\n",
        "if active_page == \"📥 Build Index\":\n",
        "    st.subheader(\"Upload PDFs → Build/Update Chroma Index\")\n",
        "\n",
        "    files = st.file_uploader(\"Upload one or more PDFs\", type=[\"pdf\"], accept_multiple_files=True)\n",
        "\n",
        "    # NEW: reset toggle\n",
        "    reset_index = st.checkbox(\"Reset index (rebuild from scratch)\", value=False, help=\"Deletes existing Chroma data in the chosen persist dir before indexing.\")\n",
        "\n",
        "    colA, colB = st.columns(2)\n",
        "    with colA:\n",
        "        if st.button(\"Build / Update Index\", use_container_width=True):\n",
        "            if not files:\n",
        "                st.error(\"Please upload at least one PDF\")\n",
        "            else:\n",
        "                with st.spinner(\"Indexing...\"):\n",
        "                    paths=[]\n",
        "                    for f in files:\n",
        "                        p = os.path.join(tempfile.gettempdir(), f.name)\n",
        "                        with open(p,\"wb\") as w: w.write(f.getbuffer())\n",
        "                        paths.append(p)\n",
        "                    docs = load_pdfs_as_docs(paths)\n",
        "                    # Faster defaults come from rag_core; still good to match them here:\n",
        "                    chunks = chunk_texts(docs, chunk_size=1800, chunk_overlap=150)\n",
        "                    _ = build_or_update_chroma(chunks, persist_dir=persist_dir, collection_name=collection, reset=reset_index)\n",
        "                    st.session_state.vs_ready = True\n",
        "                st.success(f\"Indexed {len(chunks)} chunks → {persist_dir}\")\n",
        "\n",
        "    with colB:\n",
        "        if st.button(\"Load Existing Index\", use_container_width=True):\n",
        "            try:\n",
        "                _ = load_chroma(persist_dir=persist_dir, collection_name=collection)\n",
        "                st.session_state.vs_ready = True\n",
        "                st.success(\"Loaded existing Chroma index.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Failed to load: {e}\")\n",
        "\n",
        "elif active_page == \"❓ Q&A\":\n",
        "    need_vs()\n",
        "    st.subheader(\"Ask a question (grounded in your PDFs)\")\n",
        "    q = st.text_input(\"Your question\", placeholder=\"e.g., Full procedure to publish a contract workspace\")\n",
        "    detailed = st.checkbox(\"Detailed (multi-pass map-reduce)\", value=True)\n",
        "    batch_size = st.slider(\"Batch size (Detailed mode)\", 3, 12, 6)\n",
        "\n",
        "    if st.button(\"Answer\", use_container_width=True):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            vs = load_chroma(persist_dir=persist_dir, collection_name=collection)\n",
        "            try:\n",
        "                if detailed:\n",
        "                    out = rag_answer_exhaustive(\n",
        "                        vs, q, k=max(24, top_k*2), batch=batch_size,\n",
        "                        temperature=0.0, max_tokens=min(1200, max_tokens)\n",
        "                    )\n",
        "                    mode = \"qna_exhaustive\"\n",
        "                else:\n",
        "                    out = rag_answer(vs, q, k=top_k, temperature=temperature, max_tokens=max_tokens)\n",
        "                    mode = \"qna\"\n",
        "\n",
        "                st.markdown(\"### Answer\")\n",
        "                st.write(out[\"answer\"])\n",
        "                with st.expander(\"Citations\"):\n",
        "                    for c in out[\"citations\"]:\n",
        "                        st.write(f\"[{c['slot']}] {c['source']} (p.{c['page']})\")\n",
        "\n",
        "                log_interaction(\n",
        "                    session_id=\"ui\", user=st.session_state.username,\n",
        "                    question=q, mode=mode,\n",
        "                    model=os.environ.get(\"GROQ_MODEL_OVERRIDE\",\"llama-3.3-70b-versatile\"),\n",
        "                    top_k=top_k, max_tokens=max_tokens,\n",
        "                    temperature=(temperature if mode==\"qna\" else 0.0),\n",
        "                    answer=out[\"answer\"], citations=out[\"citations\"], error=None\n",
        "                )\n",
        "            except Exception as e:\n",
        "                st.error(str(e))\n",
        "                log_interaction(\n",
        "                    session_id=\"ui\", user=st.session_state.username,\n",
        "                    question=q, mode=\"qna\" if not detailed else \"qna_exhaustive\",\n",
        "                    model=os.environ.get(\"GROQ_MODEL_OVERRIDE\",\"llama-3.3-70b-versatile\"),\n",
        "                    top_k=top_k, max_tokens=max_tokens,\n",
        "                    temperature=(temperature if not detailed else 0.0),\n",
        "                    answer=\"\", citations=[], error=str(e)\n",
        "                )\n",
        "\n",
        "elif active_page == \"💬 Document Chat\":\n",
        "    need_vs()\n",
        "    st.subheader(\"Document Chat\")\n",
        "\n",
        "    user_msg = st.text_input(\"Message\", placeholder=\"Ask a follow-up question…\")\n",
        "    col1, col2, col3 = st.columns([2,1,1])\n",
        "    with col1:\n",
        "        send = st.button(\"Send\", use_container_width=True)\n",
        "    with col2:\n",
        "        if st.button(\"Clear Chat (session)\"):\n",
        "            st.session_state.messages = []\n",
        "            st.success(\"Cleared in-memory chat history.\")\n",
        "    with col3:\n",
        "        chat_history_txt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in st.session_state.messages])\n",
        "        st.download_button(\"Download Chat (TXT)\", chat_history_txt, file_name=\"chat_history.txt\")\n",
        "\n",
        "    if send and user_msg.strip():\n",
        "        st.session_state.messages.append({\"role\":\"user\",\"content\":user_msg})\n",
        "        convo = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in st.session_state.messages[-6:]])\n",
        "        q2 = f\"Conversation so far:\\n{convo}\\n\\nUser's latest question: {user_msg}\"\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            vs = load_chroma(persist_dir=persist_dir, collection_name=collection)\n",
        "            try:\n",
        "                out = rag_answer(vs, q2, k=top_k, temperature=temperature, max_tokens=max_tokens)\n",
        "                st.session_state.messages.append({\"role\":\"assistant\",\"content\":out[\"answer\"]})\n",
        "                log_interaction(\n",
        "                    session_id=\"ui\", user=st.session_state.username,\n",
        "                    question=user_msg, mode=\"chat\",\n",
        "                    model=os.environ.get(\"GROQ_MODEL_OVERRIDE\",\"llama-3.3-70b-versatile\"),\n",
        "                    top_k=top_k, max_tokens=max_tokens, temperature=temperature,\n",
        "                    answer=out[\"answer\"], citations=out[\"citations\"], error=None\n",
        "                )\n",
        "            except Exception as e:\n",
        "                st.error(str(e))\n",
        "                log_interaction(\n",
        "                    session_id=\"ui\", user=st.session_state.username,\n",
        "                    question=user_msg, mode=\"chat\",\n",
        "                    model=os.environ.get(\"GROQ_MODEL_OVERRIDE\",\"llama-3.3-70b-versatile\"),\n",
        "                    top_k=top_k, max_tokens=max_tokens, temperature=temperature,\n",
        "                    answer=\"\", citations=[], error=str(e)\n",
        "                )\n",
        "\n",
        "    st.markdown(\"#### Recent Chat\")\n",
        "    for m in st.session_state.messages[-12:]:\n",
        "        if m[\"role\"] == \"user\":\n",
        "            st.markdown(f\"<div class='chat-bubble-user'><b>You:</b> {m['content']}</div>\", unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.markdown(f\"<div class='chat-bubble-assistant'><b>Assistant:</b> {m['content']}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "elif active_page == \"📝 Summarize\":\n",
        "    need_vs()\n",
        "    st.subheader(\"Summarize (chapter/page-level brief)\")\n",
        "    topic = st.text_input(\"(Optional) Topic/file hint\", value=\"overview\")\n",
        "    if st.button(\"Summarize\", use_container_width=True):\n",
        "        with st.spinner(\"Summarizing...\"):\n",
        "            vs = load_chroma(persist_dir=persist_dir, collection_name=collection)\n",
        "            s = summarize(vs, topic_hint=topic, k=20, max_tokens=max_tokens)\n",
        "            st.markdown(\"### Summary\")\n",
        "            st.write(s)\n",
        "            log_interaction(\n",
        "                session_id=\"ui\", user=st.session_state.username,\n",
        "                question=f\"[SUMMARY] {topic}\", mode=\"summary\",\n",
        "                model=os.environ.get(\"GROQ_MODEL_OVERRIDE\",\"llama-3.3-70b-versatile\"),\n",
        "                top_k=20, max_tokens=max_tokens, temperature=0.2,\n",
        "                answer=s, citations=[], error=None\n",
        "            )\n",
        "\n",
        "elif active_page == \"🧩 Quiz\":\n",
        "    need_vs()\n",
        "    st.subheader(\"Generate MCQ quiz\")\n",
        "    topic = st.text_input(\"(Optional) Topic hint\", placeholder=\"e.g., PR→PO flow\")\n",
        "    num = st.slider(\"Number of questions\", 3, 15, 5)\n",
        "    if st.button(\"Create Quiz\", use_container_width=True):\n",
        "        with st.spinner(\"Generating...\"):\n",
        "            vs = load_chroma(persist_dir=persist_dir, collection_name=collection)\n",
        "            items = quiz(vs, topic_hint=topic, num=num)\n",
        "            if isinstance(items, list) and items and isinstance(items[0], dict) and \"question\" in items[0]:\n",
        "                for i, qx in enumerate(items, start=1):\n",
        "                    st.markdown(f\"**Q{i}. {qx['question']}**\")\n",
        "                    for opt in qx[\"options\"]:\n",
        "                        st.write(f\"- {opt}\")\n",
        "                    st.caption(f\"**Answer:** {qx['answer']} — {qx.get('why','')}\")\n",
        "            else:\n",
        "                st.write(\"Model returned non-JSON output:\")\n",
        "                st.code(items, language=\"json\")\n",
        "            log_interaction(\n",
        "                session_id=\"ui\", user=st.session_state.username,\n",
        "                question=f\"[QUIZ] {topic} (n={num})\", mode=\"quiz\",\n",
        "                model=os.environ.get(\"GROQ_MODEL_OVERRIDE\",\"llama-3.3-70b-versatile\"),\n",
        "                top_k=top_k, max_tokens=max_tokens, temperature=0.2,\n",
        "                answer=json.dumps(items, ensure_ascii=False), citations=[], error=None\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Streamlit**"
      ],
      "metadata": {
        "id": "Qity4YgVH-5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensures the UI runs on a known version for consistent behavior.\n",
        "Avoids surprise breakages from upstream updates."
      ],
      "metadata": {
        "id": "9VofxKU9ID6N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-0Uzwe8ZOLY",
        "outputId": "9db69858-c3f4-4d90-9082-f8683e2bba0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.36.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit==1.35.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " (Environment/Service Prep)"
      ],
      "metadata": {
        "id": "FbgavMXAIJH6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhQDRKfQZb4f",
        "outputId": "57cdec9e-fb5b-417f-8bdc-ae7d0357b6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GROQ key set: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "print(\"GROQ key set:\", bool(os.environ.get(\"GROQ_API_KEY\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run App + Public Tunnel (Cloudflared)**"
      ],
      "metadata": {
        "id": "zVYhgNwOIMUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Launches Streamlit headless and opens a public URL via Cloudflared.\n",
        "Perfect for quick demos, remote reviews, and mobile testing."
      ],
      "metadata": {
        "id": "Po2HuCroIOGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, time, re, os\n",
        "from IPython.display import display, HTML  # NEW: pretty banner\n",
        "\n",
        "workdir = \"/content\"  # change if app.py is elsewhere\n",
        "\n",
        "# 1) Start Streamlit\n",
        "streamlit_proc = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"],\n",
        "    cwd=workdir,\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "\n",
        "# 2) Download cloudflared binary\n",
        "!wget -q -O /content/cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x /content/cloudflared\n",
        "\n",
        "# 3) Start the tunnel and print the URL\n",
        "tunnel_proc = subprocess.Popen(\n",
        "    [\"/content/cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8501\", \"--no-autoupdate\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "\n",
        "public_url = None\n",
        "start_time = time.time()\n",
        "print(\"Starting Streamlit + Cloudflare tunnel...\")\n",
        "\n",
        "while time.time() - start_time < 120:\n",
        "    line = tunnel_proc.stdout.readline()\n",
        "    if not line:\n",
        "        time.sleep(1); continue\n",
        "    line = line.strip()\n",
        "    print(line)\n",
        "    if \"trycloudflare.com\" in line:\n",
        "        m = re.search(r\"(https://[a-z0-9-]+\\.trycloudflare\\.com)\", line)\n",
        "        if m:\n",
        "            public_url = m.group(1)\n",
        "            print(\"\\nPublic URL:\", public_url)\n",
        "\n",
        "            # NEW: read a few more lines so your banner lands AFTER the\n",
        "            # \"Registered tunnel connection ...\" message (if/when it appears)\n",
        "            tail_deadline = time.time() + 8  # wait up to 8s for that line\n",
        "            while time.time() < tail_deadline:\n",
        "                tail = tunnel_proc.stdout.readline()\n",
        "                if not tail:\n",
        "                    time.sleep(0.2); continue\n",
        "                tail = tail.strip()\n",
        "                print(tail)\n",
        "                if \"Registered tunnel connection\" in tail:\n",
        "                    break\n",
        "\n",
        "            # NEW: big, bold, colored, clickable banner\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style=\"margin-top:12px; text-align:center;\">\n",
        "              <div style=\"\n",
        "                display:inline-block;\n",
        "                font-size:26px; font-weight:800;\n",
        "                color:#0B57D0; background:#FFF3CD;\n",
        "                border:2px solid #FCD34D; border-radius:12px;\n",
        "                padding:14px 18px; box-shadow:0 2px 10px rgba(0,0,0,0.08);\n",
        "              \">\n",
        "                🚀 <span style=\"color:#111827;\">Click to open Streamlit:</span>\n",
        "                <a href=\"{public_url}\" target=\"_blank\" style=\"color:#DC2626; text-decoration:none; font-weight:900;\">\n",
        "                  {public_url}\n",
        "                </a>\n",
        "              </div>\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "            break\n",
        "\n",
        "if not public_url:\n",
        "    print(\"\\nIf you don't see the URL above, re-run this cell and keep it running.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "CGexHJX0Ix40",
        "outputId": "f3abb996-4c6b-4e04-c983-1d306ba7af38"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit + Cloudflare tunnel...\n",
            "2025-08-12T03:32:34Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2025-08-12T03:32:34Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2025-08-12T03:32:37Z INF +--------------------------------------------------------------------------------------------+\n",
            "2025-08-12T03:32:37Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2025-08-12T03:32:37Z INF |  https://engines-preliminary-cope-been.trycloudflare.com                                   |\n",
            "\n",
            "Public URL: https://engines-preliminary-cope-been.trycloudflare.com\n",
            "2025-08-12T03:32:37Z INF +--------------------------------------------------------------------------------------------+\n",
            "2025-08-12T03:32:37Z INF Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "2025-08-12T03:32:37Z INF Version 2025.8.0 (Checksum c7d3a69da0f7b9b1bc1ddcb0597d3552bcd7c15f8bbaba463dc489b94b7544ee)\n",
            "2025-08-12T03:32:37Z INF GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "2025-08-12T03:32:37Z INF Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "2025-08-12T03:32:37Z INF Generated Connector ID: d1f6caf3-c204-4b2b-ac81-8418f3c752fd\n",
            "2025-08-12T03:32:37Z INF Initial protocol quic\n",
            "2025-08-12T03:32:37Z INF ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "2025-08-12T03:32:37Z INF ICMP proxy will use :: as source for IPv6\n",
            "2025-08-12T03:32:37Z ERR Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable originCertPath=\n",
            "2025-08-12T03:32:37Z INF ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "2025-08-12T03:32:37Z INF ICMP proxy will use :: as source for IPv6\n",
            "2025-08-12T03:32:37Z INF Starting metrics server on 127.0.0.1:20241/metrics\n",
            "2025-08-12T03:32:37Z INF Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] connIndex=0 event=0 ip=198.41.192.47\n",
            "2025/08/12 03:32:37 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "2025-08-12T03:32:38Z INF Registered tunnel connection connIndex=0 connection=ca74d048-c9e3-4c9e-b558-1b840256daf5 event=0 ip=198.41.192.47 location=ord10 protocol=quic\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <div style=\"margin-top:12px; text-align:center;\">\n",
              "              <div style=\"\n",
              "                display:inline-block;\n",
              "                font-size:26px; font-weight:800;\n",
              "                color:#0B57D0; background:#FFF3CD;\n",
              "                border:2px solid #FCD34D; border-radius:12px;\n",
              "                padding:14px 18px; box-shadow:0 2px 10px rgba(0,0,0,0.08);\n",
              "              \">\n",
              "                🚀 <span style=\"color:#111827;\">Click to open Streamlit:</span>\n",
              "                <a href=\"https://engines-preliminary-cope-been.trycloudflare.com\" target=\"_blank\" style=\"color:#DC2626; text-decoration:none; font-weight:900;\">\n",
              "                  https://engines-preliminary-cope-been.trycloudflare.com\n",
              "                </a>\n",
              "              </div>\n",
              "            </div>\n",
              "            "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}